{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JrY_bs_4SrZ"
   },
   "source": [
    "##Preliminary Notes\n",
    "\n",
    "The aim of the InCrediblAE shared task is to build your own custom attack method that will generate adversarial examples to fool a victim classifier. This notebook is intended as an easy way for you to get started.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Using GPU\n",
    "It is recommended that you run this notebook with a GPU. To do this, click on \"additional connection options\" (next to Connect / RAM usage), select \"change runtime type\", and select a GPU.\n",
    "\n",
    "<br>\n",
    "\n",
    "### (optional) Mounting Google Drive - don't bother with this if running this notebook for first time\n",
    "If you will be re-running this notebook many times, it might be convenient to mount your personal google drive. This will allow you to\n",
    "1. load data/victim files quickly rather than re-downloading them with each session\n",
    "2. save output files to a permanent location\n",
    "\n",
    "Instructions for mounting are in the 'Making your own attack section'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xKLR-legJ_e"
   },
   "source": [
    "# Setup (installing dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENZHhvJQUbFR"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/piotrmp/BODEGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JH3bxpX2UrRl",
    "outputId": "e7501646-4dda-4ff6-9c68-767321b0160d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: OpenAttack in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
      "Requirement already satisfied: nltk>=3.5 in /usr/local/lib/python3.10/dist-packages (from OpenAttack) (3.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from OpenAttack) (1.25.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from OpenAttack) (2.19.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from OpenAttack) (4.66.2)\n",
      "Requirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from OpenAttack) (4.40.0)\n",
      "Requirement already satisfied: torch>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from OpenAttack) (2.2.1+cu121)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.5->OpenAttack) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.5->OpenAttack) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.5->OpenAttack) (2023.12.25)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.1->OpenAttack) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.1->OpenAttack) (4.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.1->OpenAttack) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.1->OpenAttack) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.1->OpenAttack) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.1->OpenAttack) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.1->OpenAttack) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.1->OpenAttack) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.1->OpenAttack) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.1->OpenAttack) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.1->OpenAttack) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.1->OpenAttack) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.1->OpenAttack) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.1->OpenAttack) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.1->OpenAttack) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.1->OpenAttack) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.1->OpenAttack) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5.1->OpenAttack) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.5.1->OpenAttack) (12.4.127)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.0->OpenAttack) (0.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.0->OpenAttack) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.0->OpenAttack) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.0->OpenAttack) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.0->OpenAttack) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.0->OpenAttack) (0.4.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->OpenAttack) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->OpenAttack) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->OpenAttack) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->OpenAttack) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->OpenAttack) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->OpenAttack) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->OpenAttack) (3.9.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->OpenAttack) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->OpenAttack) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->OpenAttack) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->OpenAttack) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->OpenAttack) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->OpenAttack) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.0->OpenAttack) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.0->OpenAttack) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.0->OpenAttack) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.0->OpenAttack) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.5.1->OpenAttack) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->OpenAttack) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->OpenAttack) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->OpenAttack) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.5.1->OpenAttack) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->OpenAttack) (1.16.0)\n",
      "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (0.6.2)\n",
      "Requirement already satisfied: bert-score in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.2.1+cu121)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.0.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.40.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert-score) (1.25.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.66.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score) (24.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2024.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (4.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert-score) (12.4.127)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.22.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.4.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Collecting git+https://github.com/lucadiliello/bleurt-pytorch.git\n",
      "  Cloning https://github.com/lucadiliello/bleurt-pytorch.git to /tmp/pip-req-build-0g8aqk3u\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/lucadiliello/bleurt-pytorch.git /tmp/pip-req-build-0g8aqk3u\n",
      "  Resolved https://github.com/lucadiliello/bleurt-pytorch.git to commit 279ca1bb4106bde5a89f0f82723197e23d8446cb\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from bleurt-pytorch==0.0.1) (2.2.1+cu121)\n",
      "Requirement already satisfied: transformers>=4.25 in /usr/local/lib/python3.10/dist-packages (from bleurt-pytorch==0.0.1) (4.40.0)\n",
      "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.10/dist-packages (from bleurt-pytorch==0.0.1) (0.1.99)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->bleurt-pytorch==0.0.1) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->bleurt-pytorch==0.0.1) (4.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->bleurt-pytorch==0.0.1) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->bleurt-pytorch==0.0.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->bleurt-pytorch==0.0.1) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->bleurt-pytorch==0.0.1) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->bleurt-pytorch==0.0.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->bleurt-pytorch==0.0.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->bleurt-pytorch==0.0.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->bleurt-pytorch==0.0.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->bleurt-pytorch==0.0.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->bleurt-pytorch==0.0.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->bleurt-pytorch==0.0.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->bleurt-pytorch==0.0.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->bleurt-pytorch==0.0.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->bleurt-pytorch==0.0.1) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->bleurt-pytorch==0.0.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->bleurt-pytorch==0.0.1) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10->bleurt-pytorch==0.0.1) (12.4.127)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25->bleurt-pytorch==0.0.1) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25->bleurt-pytorch==0.0.1) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25->bleurt-pytorch==0.0.1) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25->bleurt-pytorch==0.0.1) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25->bleurt-pytorch==0.0.1) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25->bleurt-pytorch==0.0.1) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25->bleurt-pytorch==0.0.1) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25->bleurt-pytorch==0.0.1) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25->bleurt-pytorch==0.0.1) (4.66.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->bleurt-pytorch==0.0.1) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25->bleurt-pytorch==0.0.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25->bleurt-pytorch==0.0.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25->bleurt-pytorch==0.0.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25->bleurt-pytorch==0.0.1) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->bleurt-pytorch==0.0.1) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install OpenAttack\n",
    "%pip install editdistance\n",
    "%pip install bert-score\n",
    "%pip install git+https://github.com/lucadiliello/bleurt-pytorch.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hsiyq7oB-JSc"
   },
   "outputs": [],
   "source": [
    "# !git clone https://gitlab.clarin-pl.eu/syntactic-tools/lambo.git\n",
    "# %pip install ./lambo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RePvJQ37vyrt",
    "outputId": "dbfa478f-5269-4286-86bd-f7ca86dbb2ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ys5EJ9gPnWYy"
   },
   "source": [
    "# Downloading victim models and data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wc4VHWqVIXgJ"
   },
   "source": [
    "Data and models are downloaded by cloning the [clef2024-checkthat repo](https://gitlab.com/checkthat_lab/clef2024-checkthat-lab.git)\n",
    "* alternative [google drive folder link](https://drive.google.com/drive/folders/1ZsDHSejiv4USae0viTsfeLpvqXdeq0FL?usp=sharing)\n",
    "\n",
    "Data and models are downloaded then moved to /content/BODEGA/incrediblAE_public_release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5P__1F-kd6K"
   },
   "outputs": [],
   "source": [
    "# # temporary folder for downloading victim models and data\n",
    "# ! mkdir /content/clef2024-checkthat-lab\n",
    "\n",
    "# import os, sys\n",
    "# os.chdir(\"/content/clef2024-checkthat-lab\")\n",
    "\n",
    "# ! git init\n",
    "# ! git remote add -f origin https://gitlab.com/checkthat_lab/clef2024-checkthat-lab.git\n",
    "# ! git sparse-checkout init\n",
    "# ! git sparse-checkout set \"task6/incrediblAE_public_release\"\n",
    "# ! git pull origin main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6IaYgmrhkLn"
   },
   "outputs": [],
   "source": [
    "# # move downloaded files to /content/BODEGA\n",
    "# ! mv /content/clef2024-checkthat-lab/task6/incrediblAE_public_release /content/BODEGA/incrediblAE_public_release\n",
    "\n",
    "# ! mv /content/BODEGA /content/drive/MyDrive/BODEGA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAErnXyL2ebq"
   },
   "source": [
    "Misc set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEtcqKe4iLqs"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61iSmbemrAzf",
    "outputId": "a8772f59-6f29-4b5d-ad93-e37426388feb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/content/drive/MyDrive/BODEGA/outputs’: File exists\n"
     ]
    }
   ],
   "source": [
    "#folder for storing results of attack method\n",
    "! mkdir /content/drive/MyDrive/BODEGA/outputs\n",
    "\n",
    "#code below assumes we are working from the BODEGA repo\n",
    "os.chdir(\"/content/drive/MyDrive/BODEGA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPy0_QH405Xk"
   },
   "source": [
    "# Making your own attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4jxt8Ok1Evm"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUdKAU1K1IUH"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import OpenAttack\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "from OpenAttack.tags import Tag\n",
    "from OpenAttack.text_process.tokenizer import PunctTokenizer\n",
    "\n",
    "from metrics.BODEGAScore import BODEGAScore\n",
    "from utils.data_mappings import dataset_mapping, dataset_mapping_pairs, SEPARATOR_CHAR\n",
    "from utils.no_ssl_verify import no_ssl_verify\n",
    "from victims.bert import VictimBERT, readfromfile_generator\n",
    "from victims.bilstm import VictimBiLSTM\n",
    "from victims.caching import VictimCache\n",
    "from victims.unk_fix_wrapper import UNK_TEXT\n",
    "\n",
    "#imports for BodegaAttackEval wrapper\n",
    "from typing import Any, Dict, Generator, Iterable, List, Optional, Union\n",
    "from tqdm import tqdm\n",
    "from OpenAttack.utils import visualizer, result_visualizer, get_language, language_by_name\n",
    "from OpenAttack.tags import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mrfh0t9mA5Ge",
    "outputId": "355e54c2-de67-4007-e3b5-d4492335a316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda device available False\n"
     ]
    }
   ],
   "source": [
    "using_mounted_drive = False\n",
    "print('Cuda device available', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zi_zkYknAqiY"
   },
   "source": [
    "## (do not change) Wrapper for producing submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9Dt2wyYAcfB"
   },
   "outputs": [],
   "source": [
    "class BodegaAttackEval(OpenAttack.AttackEval):\n",
    "  '''\n",
    "  wrapper for OpenAttack.AttackEval to produce a submission.tsv file for shared task evaluation\n",
    "\n",
    "  To perform evaluation, we use a new method: eval_and_save_tsv() rather than the usual AttackEval.eval()\n",
    "  submission.tsv file consists of 4 columns for each sample in attack set: succeeded, num_queries, original_text and modified text (newlines are escaped)\n",
    "\n",
    "  '''\n",
    "  def eval_and_save_tsv(self, dataset: Iterable[Dict[str, Any]], total_len : Optional[int] = None, visualize : bool = False, progress_bar : bool = False, num_workers : int = 0, chunk_size : Optional[int] = None, tsv_file_path: Optional[os.PathLike] = None):\n",
    "      \"\"\"\n",
    "      Evaluation function of `AttackEval`.\n",
    "\n",
    "      Args:\n",
    "          dataset: An iterable dataset.\n",
    "          total_len: Total length of dataset (will be used if dataset doesn't has a `__len__` attribute).\n",
    "          visualize: Display a pretty result for each data in the dataset.\n",
    "          progress_bar: Display a progress bar if `True`.\n",
    "          num_workers: The number of processes running the attack algorithm. Default: 0 (running on the main process).\n",
    "          chunk_size: Processing pool trunks size.\n",
    "\n",
    "          tsv_file_path: path to save submission tsv\n",
    "\n",
    "      Returns:\n",
    "          A dict of attack evaluation summaries.\n",
    "\n",
    "      \"\"\"\n",
    "\n",
    "\n",
    "      if hasattr(dataset, \"__len__\"):\n",
    "          total_len = len(dataset)\n",
    "\n",
    "      def tqdm_writer(x):\n",
    "          return tqdm.write(x, end=\"\")\n",
    "\n",
    "      if progress_bar:\n",
    "          result_iterator = tqdm(self.ieval(dataset, num_workers, chunk_size), total=total_len)\n",
    "      else:\n",
    "          result_iterator = self.ieval(dataset, num_workers, chunk_size)\n",
    "\n",
    "      total_result = {}\n",
    "      total_result_cnt = {}\n",
    "      total_inst = 0\n",
    "      success_inst = 0\n",
    "\n",
    "      #list for tsv\n",
    "      x_orig_list = []\n",
    "      x_adv_list = []\n",
    "      num_queries_list = []\n",
    "      succeed_list = []\n",
    "\n",
    "      # Begin for\n",
    "      for i, res in enumerate(result_iterator):\n",
    "          total_inst += 1\n",
    "          success_inst += int(res[\"success\"])\n",
    "\n",
    "          if TAG_Classification in self.victim.TAGS:\n",
    "              x_orig = res[\"data\"][\"x\"]\n",
    "              if res[\"success\"]:\n",
    "                  x_adv = res[\"result\"]\n",
    "                  if Tag(\"get_prob\", \"victim\") in self.victim.TAGS:\n",
    "                      self.victim.set_context(res[\"data\"], None)\n",
    "                      try:\n",
    "                          probs = self.victim.get_prob([x_orig, x_adv])\n",
    "                      finally:\n",
    "                          self.victim.clear_context()\n",
    "                      y_orig = probs[0]\n",
    "                      y_adv = probs[1]\n",
    "                  elif Tag(\"get_pred\", \"victim\") in self.victim.TAGS:\n",
    "                      self.victim.set_context(res[\"data\"], None)\n",
    "                      try:\n",
    "                          preds = self.victim.get_pred([x_orig, x_adv])\n",
    "                      finally:\n",
    "                          self.victim.clear_context()\n",
    "                      y_orig = int(preds[0])\n",
    "                      y_adv = int(preds[1])\n",
    "                  else:\n",
    "                      raise RuntimeError(\"Invalid victim model\")\n",
    "              else:\n",
    "                  y_adv = None\n",
    "                  x_adv = None\n",
    "                  if Tag(\"get_prob\", \"victim\") in self.victim.TAGS:\n",
    "                      self.victim.set_context(res[\"data\"], None)\n",
    "                      try:\n",
    "                          probs = self.victim.get_prob([x_orig])\n",
    "                      finally:\n",
    "                          self.victim.clear_context()\n",
    "                      y_orig = probs[0]\n",
    "                  elif Tag(\"get_pred\", \"victim\") in self.victim.TAGS:\n",
    "                      self.victim.set_context(res[\"data\"], None)\n",
    "                      try:\n",
    "                          preds = self.victim.get_pred([x_orig])\n",
    "                      finally:\n",
    "                          self.victim.clear_context()\n",
    "                      y_orig = int(preds[0])\n",
    "                  else:\n",
    "                      raise RuntimeError(\"Invalid victim model\")\n",
    "              info = res[\"metrics\"]\n",
    "              info[\"Succeed\"] = res[\"success\"]\n",
    "              if visualize:\n",
    "                  if progress_bar:\n",
    "                      visualizer(i + 1, x_orig, y_orig, x_adv, y_adv, info, tqdm_writer, self.tokenizer)\n",
    "                  else:\n",
    "                      visualizer(i + 1, x_orig, y_orig, x_adv, y_adv, info, sys.stdout.write, self.tokenizer)\n",
    "\n",
    "              #list for tsv\n",
    "              succeed_list.append(res[\"success\"])\n",
    "              num_queries_list.append(res[\"metrics\"][\"Victim Model Queries\"])\n",
    "              x_orig_list.append(x_orig)\n",
    "\n",
    "              if res[\"success\"]:\n",
    "                x_adv_list.append(x_adv)\n",
    "              else:\n",
    "                x_adv_list.append(\"ATTACK_UNSUCCESSFUL\")\n",
    "\n",
    "\n",
    "\n",
    "          for kw, val in res[\"metrics\"].items():\n",
    "              if val is None:\n",
    "                  continue\n",
    "\n",
    "              if kw not in total_result_cnt:\n",
    "                  total_result_cnt[kw] = 0\n",
    "                  total_result[kw] = 0\n",
    "              total_result_cnt[kw] += 1\n",
    "              total_result[kw] += float(val)\n",
    "      # End for\n",
    "\n",
    "      summary = {}\n",
    "      summary[\"Total Attacked Instances\"] = total_inst\n",
    "      summary[\"Successful Instances\"] = success_inst\n",
    "      summary[\"Attack Success Rate\"] = success_inst / total_inst\n",
    "      for kw in total_result_cnt.keys():\n",
    "          if kw in [\"Succeed\"]:\n",
    "              continue\n",
    "          if kw in [\"Query Exceeded\"]:\n",
    "              summary[\"Total \" + kw] = total_result[kw]\n",
    "          else:\n",
    "              summary[\"Avg. \" + kw] = total_result[kw] / total_result_cnt[kw]\n",
    "\n",
    "      if visualize:\n",
    "          result_visualizer(summary, sys.stdout.write)\n",
    "\n",
    "\n",
    "      #saving tsv\n",
    "      if tsv_file_path is not None:\n",
    "        with open(tsv_file_path, 'w') as f:\n",
    "          f.write('succeeded' + '\\t' + 'num_queries' + '\\t' + 'original_text' + '\\t' + 'modified_text' + '\\t'+ '\\n') #header\n",
    "          for success, num_queries, x_orig, x_adv in zip(succeed_list, num_queries_list, x_orig_list, x_adv_list):\n",
    "            escaped_x_orig = x_orig.replace('\\n', '\\\\n') #escaping newlines\n",
    "            escaped_x_adv = x_adv.replace('\\n', '\\\\n')\n",
    "            f.write(str(success) + '\\t' + str(num_queries) + '\\t' + escaped_x_orig + '\\t' + escaped_x_adv + '\\t'+ '\\n')\n",
    "\n",
    "      return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IuS87xdtFgo"
   },
   "source": [
    "## (optional) Mounting Google Drive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRG-ziUTtpVr"
   },
   "source": [
    "Steps to use mounted google drive:\n",
    "1. create a folder in your local google drive (e.g. `incrediblAE_public_release`)  \n",
    "2. download all directories from the download link (see [Download section above](https://colab.research.google.com/drive/1juHWIL44z8O3C5wDAE45vzlJgX51KI5D?authuser=3#scrollTo=eVVE2-64rKuS&line=3&uniqifier=1://)) and upload them to your google drive folder\n",
    "3. create an empty subdirectory called `outputs` (`incredibleAE_public_release/outputs/`)\n",
    "\n",
    "At this point, your google drive folder should have 6 subdirectories (C19, FC, HN, PR2, RD, and outputs)\n",
    "4. uncomment code below, replacing path_to_mounted_dir with path to your folder (e.g. `/content/drive/My Drive/incrediblAE_public_release`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFIINcQytJbD"
   },
   "outputs": [],
   "source": [
    "using_mounted_drive = True\n",
    "path_to_mounted_folder = '/content/drive/MyDrive/BODEGA/incrediblAE_public_release'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdBk06y30pLe"
   },
   "source": [
    "You can also comment out the !gdown command in Downloading section, so the notebook doesn't redownload data each time you run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_I-IPo0S1CpK"
   },
   "source": [
    "## Making custom attacker (token shuffler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hW-vDEq6AQ0T"
   },
   "source": [
    "Here's an example of how to create a custom attack method.\n",
    "Your attacker will need to subclass `OpenAttack.attackers.ClassificationAttacker`  \n",
    "\n",
    "(See also OpenAttack framework docs: https://openattack.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Yp8hGGI1kN4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This example code shows how to design a customized attack model (that shuffles the tokens in the original sentence).\n",
    "Taken from https://github.com/thunlp/OpenAttack/blob/master/examples/custom_attacker.py\n",
    "'''\n",
    "\n",
    "class BaseCaseSwapWordsAttack(OpenAttack.attackers.ClassificationAttacker):\n",
    "    @property\n",
    "    def TAGS(self):\n",
    "        # returns tags can help OpenAttack to check your parameters automatically\n",
    "        return { self.lang_tag, Tag(\"get_pred\", \"victim\") }\n",
    "\n",
    "    def __init__(self, tokenizer = None):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = PunctTokenizer()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lang_tag = OpenAttack.utils.get_language([self.tokenizer])\n",
    "        # We add parameter ``processor`` to specify the :py:class:`.TextProcessor` which is used for tokenization and detokenization.\n",
    "        # By default, :py:class:`.DefaultTextProcessor` is used.\n",
    "\n",
    "    def attack(self, victim, input_, goal):\n",
    "        # Generate a potential adversarial example\n",
    "        x_new = self.tokenizer.detokenize(\n",
    "            self.swap( self.tokenizer.tokenize(input_, pos_tagging=False) )\n",
    "        )\n",
    "\n",
    "        # Get the predictions of victim classifier\n",
    "        y_new = victim.get_pred([ x_new ])\n",
    "\n",
    "        # Check for attack goal\n",
    "        if goal.check(x_new, y_new):\n",
    "            return x_new\n",
    "        # Failed\n",
    "        return None\n",
    "\n",
    "    def swap(self, sentence):\n",
    "        # Shuffle tokens to generate a potential adversarial example\n",
    "        random.shuffle(sentence)\n",
    "\n",
    "        # Return the potential adversarial example\n",
    "        return sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KNtHzV1EyRU-",
    "outputId": "b39842de-849d-4e79-f17b-3ded2fbc0330"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npMdZlxSoGdg"
   },
   "outputs": [],
   "source": [
    "class ReplaceALL_LettersAttack(OpenAttack.attackers.ClassificationAttacker):\n",
    "    @property\n",
    "    def TAGS(self):\n",
    "        # Returns tags to help OpenAttack check your parameters automatically\n",
    "        return {self.lang_tag, Tag(\"get_pred\", \"victim\")}\n",
    "\n",
    "    def __init__(self, tokenizer=None):\n",
    "        if tokenizer is None:\n",
    "            # Assuming DefaultTextProcessor is used if no tokenizer is provided\n",
    "            tokenizer = PunctTokenizer()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lang_tag = OpenAttack.utils.get_language([self.tokenizer])\n",
    "        # Load stop words from NLTK\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def attack(self, victim, input_, goal):\n",
    "        tokens = self.tokenizer.tokenize(input_)\n",
    "        adv_tokens = self.swap(tokens, -1, replace_all=True)\n",
    "        x_new = self.tokenizer.detokenize(adv_tokens)\n",
    "        print(x_new)\n",
    "        y_new = victim.get_pred([x_new])\n",
    "\n",
    "        if goal.check(x_new, y_new):\n",
    "            return x_new\n",
    "        return None\n",
    "\n",
    "    def swap(self, tokens, k=3, replace_all=False):\n",
    "        homoglyphs = {\n",
    "            'a': ['а', 'ɑ', 'а'],\n",
    "            'e': ['е'],\n",
    "            'o': ['о', 'ο', 'о'],\n",
    "            'c': ['с', 'ϲ'],\n",
    "            'p': ['р'],\n",
    "            'x': ['х'],\n",
    "            'y': ['у'],\n",
    "            'i': ['і', 'í'],\n",
    "        }\n",
    "\n",
    "        def get_word(token):\n",
    "            return token[0] if isinstance(token, tuple) else token\n",
    "\n",
    "        candidates = [\n",
    "            token for token in tokens\n",
    "            if get_word(token).lower() not in self.stop_words and any(char in homoglyphs for char in get_word(token))\n",
    "        ]\n",
    "        selected_words = random.sample(candidates, min(k, len(candidates))) if k > 0 else candidates\n",
    "\n",
    "        for i in range(len(tokens)):\n",
    "            current_word = get_word(tokens[i])\n",
    "            if tokens[i] in selected_words:\n",
    "                new_word = []\n",
    "                for char in current_word:\n",
    "                    if char in homoglyphs:\n",
    "                        new_word.append(random.choice(homoglyphs[char]) if replace_all else homoglyphs[char][0])\n",
    "                    else:\n",
    "                        new_word.append(char)\n",
    "                if isinstance(tokens[i], tuple):\n",
    "                    tokens[i] = (\"\".join(new_word), tokens[i][1])\n",
    "                else:\n",
    "                    tokens[i] = \"\".join(new_word)\n",
    "                if not replace_all:\n",
    "                    selected_words.remove(tokens[i])\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djD2FQmlLIW4"
   },
   "outputs": [],
   "source": [
    "class ReplaceLettersAttack(OpenAttack.attackers.ClassificationAttacker):\n",
    "    @property\n",
    "    def TAGS(self):\n",
    "        # Returns tags to help OpenAttack check your parameters automatically\n",
    "        return {self.lang_tag, Tag(\"get_pred\", \"victim\")}\n",
    "\n",
    "    def __init__(self, tokenizer=None):\n",
    "        if tokenizer is None:\n",
    "            # Assuming DefaultTextProcessor is used if no tokenizer is provided\n",
    "            tokenizer = PunctTokenizer()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lang_tag = OpenAttack.utils.get_language([self.tokenizer])\n",
    "        # Load stop words from NLTK\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def attack(self, victim, input_, goal):\n",
    "        tokens = self.tokenizer.tokenize(input_)\n",
    "        adv_tokens = self.swap(tokens)\n",
    "        x_new = self.tokenizer.detokenize(adv_tokens)\n",
    "        y_new = victim.get_pred([x_new])\n",
    "\n",
    "        if goal.check(x_new, y_new):\n",
    "            return x_new\n",
    "        return None\n",
    "\n",
    "    def swap(self, tokens, k=3, replace_all=False):\n",
    "        homoglyphs = {\n",
    "            'a': ['а', 'ɑ', 'а'],\n",
    "            'e': ['е'],\n",
    "            'o': ['о', 'ο', 'о'],\n",
    "            'c': ['с', 'ϲ'],\n",
    "            'p': ['р'],\n",
    "            'x': ['х'],\n",
    "            'y': ['у'],\n",
    "            'i': ['і', 'í'],\n",
    "        }\n",
    "\n",
    "        # Define function to extract the text part of a token\n",
    "        def get_word(token):\n",
    "            return token[0] if isinstance(token, tuple) else token\n",
    "\n",
    "        # Select indices of tokens to be replaced\n",
    "        candidate_indices = [\n",
    "            i for i, token in enumerate(tokens)\n",
    "            if get_word(token).lower() not in self.stop_words and any(char in homoglyphs for char in get_word(token))\n",
    "        ]\n",
    "        selected_indices = random.sample(candidate_indices, min(k, len(candidate_indices))) if k > 0 else candidate_indices\n",
    "\n",
    "        # Replace homoglyphs in selected tokens\n",
    "        for i in selected_indices:\n",
    "            current_word = get_word(tokens[i])\n",
    "            new_word = ''.join(random.choice(homoglyphs[char]) if char in homoglyphs and replace_all else homoglyphs.get(char, [char])[0] for char in current_word)\n",
    "            if isinstance(tokens[i], tuple):\n",
    "                tokens[i] = (new_word, tokens[i][1])  # Maintain any tuple structure\n",
    "            else:\n",
    "                tokens[i] = new_word\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3FL8G82o3RS",
    "outputId": "c6193b4d-9c99-44cd-d69c-289be6ff490b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: replicate in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
      "Requirement already satisfied: httpx<1,>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from replicate) (0.27.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from replicate) (24.0)\n",
      "Requirement already satisfied: pydantic>1.10.7 in /usr/local/lib/python3.10/dist-packages (from replicate) (2.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from replicate) (4.11.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (3.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.21.0->replicate) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.21.0->replicate) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>1.10.7->replicate) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>1.10.7->replicate) (2.18.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.21.0->replicate) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "vakj7_RZqYkv",
    "outputId": "fee4cc27-4925-49e5-a876-0b0935182267"
   },
   "outputs": [],
   "source": [
    "# from google.colab import userdata\n",
    "# userdata.get('REPLICATE_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cWr8J5YBbjld"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = \"your_token_here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1B9t8lcNAySb"
   },
   "outputs": [],
   "source": [
    "import replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmAIoFWqtqMB"
   },
   "outputs": [],
   "source": [
    "class LLAMA2ParaphraseAttack(OpenAttack.attackers.ClassificationAttacker):\n",
    "    @property\n",
    "    def TAGS(self):\n",
    "        return {self.lang_tag, Tag(\"get_pred\", \"victim\")}\n",
    "\n",
    "    def __init__(self, tokenizer=None):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = PunctTokenizer()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lang_tag = OpenAttack.utils.get_language([self.tokenizer])\n",
    "\n",
    "    def attack(self, victim, input_, goal):\n",
    "        x_new = self.paraphrase_with_llama(input_)\n",
    "\n",
    "        if x_new is None:\n",
    "            return None\n",
    "\n",
    "        y_new = victim.get_pred([x_new])\n",
    "\n",
    "        if goal.check(x_new, y_new):\n",
    "            return x_new\n",
    "\n",
    "        return None\n",
    "\n",
    "    def paraphrase_with_llama(self, sentence):\n",
    "        token_count = len(sentence.split())\n",
    "\n",
    "        print(\"token count:\", token_count)\n",
    "\n",
    "        input_params = {\n",
    "            \"top_p\": 1,\n",
    "            \"prompt\": f\"Paraphrase the following sentence with similar length {token_count}: {sentence}\",\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_new_tokens\": token_count + 10\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            output = api.run(\n",
    "                \"meta/meta-llama-3-70b-instruct\",\n",
    "                input=input_params\n",
    "            )\n",
    "            print(\"\".join(output))\n",
    "            return \"\".join(output)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to paraphrase with LLAMA-2: {str(e)}\")\n",
    "            return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zzbgqP4NYfec"
   },
   "outputs": [],
   "source": [
    "class LLAMA3MistralCheckerCollabAttack(OpenAttack.attackers.ClassificationAttacker):\n",
    "\n",
    "\n",
    "#   ┌──────────────────────────────────────────────────────┐\n",
    "#   │                                                      ▼\n",
    "#   │           ┌─LLAMA3───┐   PARAPHRASED TEXT  ┌─MISTRAL─────┐  IF SAME\n",
    "# INPUT────────►│PARAPHRASE├────────────────────►│MEANING CHECK├──────────►OUTPUT\n",
    "#               │TEXT      │                     └──────┬──────┘\n",
    "#               └──────────┘                            │\n",
    "#                     ▲                                 │\n",
    "#                     │            IF NOT               │\n",
    "#                     └─────────────────────────────────┘\n",
    "\n",
    "    @property\n",
    "    def TAGS(self):\n",
    "        return {self.lang_tag, Tag(\"get_pred\", \"victim\")}\n",
    "\n",
    "    def __init__(self, tokenizer=None):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = PunctTokenizer()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lang_tag = OpenAttack.utils.get_language([self.tokenizer])\n",
    "\n",
    "    def attack(self, victim, input_, goal):\n",
    "        x_new = self.paraphrase_with_llama(input_)\n",
    "\n",
    "        if x_new is None:\n",
    "            return None\n",
    "\n",
    "        y_new = victim.get_pred([x_new])\n",
    "\n",
    "        if goal.check(x_new, y_new):\n",
    "            return x_new\n",
    "\n",
    "        return None\n",
    "\n",
    "    def paraphrase_with_llama(self, sentence):\n",
    "        token_count = len(sentence.split())\n",
    "\n",
    "        print(\"token count:\", token_count)\n",
    "\n",
    "        input_params = {\n",
    "            \"top_p\": 1,\n",
    "            \"prompt\": f\"Paraphrase the following sentence with similar length {token_count} only give sentence as output: {sentence}\",\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_new_tokens\": token_count + 10\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "          is_same_meaning = False\n",
    "          output = \"\"\n",
    "          while not is_same_meaning:\n",
    "            llama3_output = replicate.run(\n",
    "                \"meta/meta-llama-3-70b-instruct\",\n",
    "                input=input_params\n",
    "            )\n",
    "            print(\"\".join(llama3_output))\n",
    "            llama3_out = \"\".join(llama3_output)\n",
    "\n",
    "            input_mistral_params = {\n",
    "            \"top_p\": 1,\n",
    "            \"top_k\": 20,\n",
    "            \"prompt\": f\"does given sentence has same meaning as \\\"{sentence}\\\" only output \\\"yes\\\" or \\\"no\\\" nothing else: {llama3_out}\",\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_new_tokens\": 3\n",
    "            }\n",
    "\n",
    "\n",
    "            mistral_out = replicate.run(\n",
    "                \"mistralai/mistral-7b-instruct-v0.2\",\n",
    "                input=input_mistral_params\n",
    "                                  )\n",
    "            mistral_out = \"\".join(mistral_out)\n",
    "            mistral_out = mistral_out.lower()\n",
    "            print(mistral_out)\n",
    "\n",
    "            print(f\"meaning checker result:{mistral_out}\")\n",
    "\n",
    "            if \"yes\" in mistral_out:\n",
    "              print(\"found the one\")\n",
    "              output=llama3_out\n",
    "              is_same_meaning = True\n",
    "\n",
    "          return output\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to paraphrase with LLAMA-2: {str(e)}\")\n",
    "            return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XD2u3SZehR9o"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class LLAMA3StyleConverterAttack(OpenAttack.attackers.ClassificationAttacker):\n",
    "\n",
    "#             ┌─LLAMA3────┐  PARAPHRASED TEXT  ┌─LLAMA3────┐\n",
    "# INPUT──────►│PARAPHREASE├───────────────────►│MATCH STYLE├───►OUTPUT\n",
    "#   │         │TEXT       │                    │OF INPUT   │\n",
    "#   │         └───────────┘                    └───────────┘\n",
    "#   │                                                ▲\n",
    "#   │                                                │\n",
    "#   └────────────────────────────────────────────────┘\n",
    "\n",
    "    @property\n",
    "    def TAGS(self):\n",
    "        return {self.lang_tag, Tag(\"get_pred\", \"victim\")}\n",
    "\n",
    "    def __init__(self, tokenizer=None):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = PunctTokenizer()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lang_tag = OpenAttack.utils.get_language([self.tokenizer])\n",
    "\n",
    "    def attack(self, victim, input_, goal):\n",
    "        x_new = self.paraphrase_with_llama(input_)\n",
    "\n",
    "        if x_new is None:\n",
    "            return None\n",
    "\n",
    "        y_new = victim.get_pred([x_new])\n",
    "\n",
    "        if goal.check(x_new, y_new):\n",
    "            return x_new\n",
    "\n",
    "        return None\n",
    "\n",
    "    def paraphrase_with_llama(self, sentence):\n",
    "        token_count = len(sentence.split())\n",
    "\n",
    "        print(\"token count:\", token_count)\n",
    "\n",
    "        # input_params = {\n",
    "        #     \"top_p\": 1,\n",
    "        #     \"prompt\": f\"Paraphrase the following sentence with similar length {token_count} only give sentence as output: {sentence}\",\n",
    "        #     \"temperature\": 0.7,\n",
    "        #     \"max_new_tokens\": token_count + 10\n",
    "        # }\n",
    "\n",
    "        paraphraser = LLamaWrapper(prompt=f\"Paraphrase the following sentence without changing the sentence structure with similar length {token_count}. change the words with their synonyms. only give sentence as output: {sentence}\",top_p=1,top_k=5,temperature=0.7,max_new_tokens=token_count + 10)\n",
    "\n",
    "        ret = paraphraser.run()\n",
    "\n",
    "        style_matcher = LLamaWrapper(prompt=f\"Match the exact structure of the sentence \\\"{ret}\\\" to the sentence \\\"{sentence}\\\" with max length of {token_count}. only give the sentence as output.\",top_p=1,top_k=5,temperature=0.7,max_new_tokens=token_count + 10)\n",
    "        end_res = style_matcher.run()\n",
    "\n",
    "        return end_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tnf5Y7E3fz2X"
   },
   "outputs": [],
   "source": [
    "class BasakSwapWordsAttack(OpenAttack.attackers.ClassificationAttacker):\n",
    "    @property\n",
    "    def TAGS(self):\n",
    "        # returns tags can help OpenAttack to check your parameters automatically\n",
    "        return { self.lang_tag, Tag(\"get_pred\", \"victim\") }\n",
    "\n",
    "    def __init__(self, tokenizer = None):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = PunctTokenizer()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lang_tag = OpenAttack.utils.get_language([self.tokenizer])\n",
    "        # We add parameter ``processor`` to specify the :py:class:`.TextProcessor` which is used for tokenization and detokenization.\n",
    "        # By default, :py:class:`.DefaultTextProcessor` is used.\n",
    "\n",
    "    def attack(self, victim, input_, goal):\n",
    "        # Generate a potential adversarial example\n",
    "        # x_new = self.tokenizer.detokenize(\n",
    "        #     self.swap( self.tokenizer.tokenize(input_, pos_tagging=False) )\n",
    "        # )\n",
    "\n",
    "        x_new = this.change_n_words_randomly(input_,len(input_)//2)\n",
    "\n",
    "        # Get the predictions of victim classifier\n",
    "        y_new = victim.get_pred([ x_new ])\n",
    "\n",
    "        # Check for attack goal\n",
    "        if goal.check(x_new, y_new):\n",
    "            return x_new\n",
    "        # Failed\n",
    "        return None\n",
    "\n",
    "    def change_n_words_randomly(text,i):\n",
    "\n",
    "\n",
    "      lst = text.split()\n",
    "      indexes = random.sample(range(0, len(lst)), i)\n",
    "\n",
    "      if i == 1:\n",
    "          k = indexes[0]\n",
    "          l = indexes[0]\n",
    "          while l == k:\n",
    "              x = random.sample(range(0, len(lst)), i)\n",
    "              l = x[0]\n",
    "\n",
    "          #print(k,l)\n",
    "          if k >l:\n",
    "              a = lst[0:l+1] + lst[k:k+1] + lst[l+1:k] + lst[k+1:]\n",
    "              return ' '.join(a)\n",
    "          else:\n",
    "              a = lst[0:k] + lst[k+1:l+1] + lst[k:k+1] + lst[l+1:]\n",
    "              return ' '.join(a)\n",
    "\n",
    "\n",
    "\n",
    "      tnk = 0\n",
    "      if len(indexes)%2 != 0:\n",
    "          tnk=1\n",
    "\n",
    "      k = tnk\n",
    "      while k <len(indexes):\n",
    "          tmp = lst[indexes[k+1]]\n",
    "          lst[indexes[k+1]] = lst[indexes[k]]\n",
    "          lst[indexes[k]] =tmp\n",
    "          k+=2\n",
    "\n",
    "      if tnk == 1:\n",
    "          tmp = lst[indexes[0]]\n",
    "          lst[indexes[0]] = lst[indexes[len(indexes)-1]]\n",
    "          lst[indexes[len(indexes)-1]] = tmp\n",
    "\n",
    "      return ' '.join(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6T6y1sMM1-oz"
   },
   "source": [
    "## Testing your attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0I1wAsXTAEPS"
   },
   "source": [
    "The code below will test MyAttacker (above) on the victim classifier, compute BODEGA score, and output results to /content/BODEGA/outputs.\n",
    "\n",
    "WARNING: files in default output directory (/content/BODGEa/outputs) do not persist after you disconnect from the colab runtime session. To keep them, you can either:\n",
    "\n",
    "1. download them manually or\n",
    "2. set `out_dir` to a mounted Google Drive directory (will automatically save files to your google drive)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEKKD2Mf5t1n"
   },
   "source": [
    "### Choose task + victim classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCYdGtwPfLTd"
   },
   "outputs": [],
   "source": [
    "# determinism\n",
    "random.seed(10)\n",
    "torch.manual_seed(10)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Change these variables to what you want\n",
    "task = 'PR2' # PR2, HN, FC, RD, C19\n",
    "victim_model = 'BERT' # BERT or BiLSTM\n",
    "using_custom_attacker = True # change to False if you want to test out OpenAttack's pre-implemented attackers (e.g. BERTattack)\n",
    "attack = 'custom' # if using custom attack, this name can be whatever you want. If using pre-implemented attack, set to name of attacker ('BERTattack')\n",
    "\n",
    "# misc variables - no need to change\n",
    "targeted = False # this shared task evaluates performance in an untargeted scenario\n",
    "visualize_adv_examples = True # prints adversarial samples as they are generated, showing the difference between original\n",
    "using_first_n_samples = False # used when you want to evaluate on a subset of the full eval set.\n",
    "first_n_samples = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVedXFrcDfM4"
   },
   "source": [
    "### Run to evaluate attacker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CABdrAeDvOCX",
    "outputId": "b1f44542-b562-4e47-8f9f-9c37b0338d2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GhiKNGph_unn",
    "outputId": "d854fb77-9a02-4354-a8b5-b16a3d05917a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing submission file found. This script will overwrite previous file: /content/drive/MyDrive/BODEGA/incrediblAE_public_release/outputs/submission_PR2_False_custom_BERT.tsv\n",
      "Setting up the device...\n",
      "Loading up victim model...\n",
      "Victim caching: file found, loading...\n",
      "Loading data...\n",
      "Subset size: 416\n",
      "Setting up the attacker...\n",
      "Evaluating the attack...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BleurtSPTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token count: 23\n",
      "Saucier emphasized that the U.S. Constitution guarantees citizens inherent rights to protection from government persecution.\n",
      "yes. both\n",
      "meaning checker result:yes. both\n",
      "found the one\n",
      "Sample: 1 =====================================================================\n",
      "\u001b[31mLabel: 0 (98.07%) --> Failed!\u001b[0m               |                                   \n",
      "                                            | Running Time:            1.8953   \n",
      "“ The U . S . Constitution clearly states   | Query Exceeded:          no       \n",
      "that all citizens are born with inalienable | Victim Model Queries:    2        \n",
      "rights to be free from persecution by the   | BODEGA Score:            (later)  \n",
      "government ,” Saucier said .                | Succeed:                 no       \n",
      "                                            |                                   \n",
      "token count: 10\n",
      "The paper's sensationalism was also on full display.\n",
      "yes. both\n",
      "meaning checker result:yes. both\n",
      "found the one\n",
      "Sample: 2 =====================================================================\n",
      "\u001b[32mLabel: 0 (97.23%) --> 1 (99.94%)\u001b[0m            |                                   \n",
      "                                            |                                   \n",
      "\u001b[1;31mIt\u001b[0m \u001b[1;31mwas\u001b[0m \u001b[1;31malso\u001b[0m \u001b[1;31mapparently\u001b[0m \u001b[1;31mevidence\u001b[0m \u001b[1;31mof\u001b[0m          |                                   \n",
      "\u001b[1;32m  \u001b[0m \u001b[1;32m   \u001b[0m \u001b[1;32mThe \u001b[0m \u001b[1;32mpaper     \u001b[0m \u001b[1;32m'       \u001b[0m \u001b[1;32ms \u001b[0m          | Running Time:            1.2487   \n",
      "                                            | Query Exceeded:          no       \n",
      "\u001b[1;31mthe           \u001b[0m \u001b[1;31mpaper\u001b[0m \u001b[1;31m’   \u001b[0m \u001b[1;31ms \u001b[0m \u001b[1;31mclickbait\u001b[0m      | Victim Model Queries:    3        \n",
      "\u001b[1;32msensationalism\u001b[0m \u001b[1;32mwas  \u001b[0m \u001b[1;32malso\u001b[0m \u001b[1;32mon\u001b[0m \u001b[1;32mfull     \u001b[0m      | BODEGA Score:            (later)  \n",
      "                                            | Succeed:                 yes      \n",
      "\u001b[1;31mtendencies\u001b[0m \u001b[1;31m:\u001b[0m                                |                                   \n",
      "\u001b[1;32mdisplay   \u001b[0m \u001b[1;32m.\u001b[0m                                |                                   \n",
      "                                            |                                   \n",
      "token count: 17\n",
      "Ebola virus disease, once called Ebola haemorrhagic fever, is a deadly and frequently lethal human illness.\n",
      "yes, both\n",
      "meaning checker result:yes, both\n",
      "found the one\n",
      "Sample: 3 =====================================================================\n",
      "\u001b[31mLabel: 0 (99.95%) --> Failed!\u001b[0m               |                                   \n",
      "                                            | Running Time:            1.7722   \n",
      "Ebola virus disease , formerly known as     | Query Exceeded:          no       \n",
      "Ebola haemorrhagic fever , is a severe ,    | Victim Model Queries:    2        \n",
      "often fatal illness in humans .             | BODEGA Score:            (later)  \n",
      "                                            | Succeed:                 no       \n",
      "                                            |                                   \n",
      "token count: 16\n",
      "Ricca's behavior was so outrageous that Uruguay's papal ambassador had to step in to get him fired.\n",
      "yes. both\n",
      "meaning checker result:yes. both\n",
      "found the one\n",
      "Sample: 4 =====================================================================\n",
      "\u001b[31mLabel: 0 (99.97%) --> Failed!\u001b[0m               |                                   \n",
      "                                            | Running Time:            1.801    \n",
      "So flagrant was Ricca ’ s behaviour that it | Query Exceeded:          no       \n",
      "took intervention by Uruguay ’ s nuncio to  | Victim Model Queries:    2        \n",
      "have him removed .                          | BODEGA Score:            (later)  \n",
      "                                            | Succeed:                 no       \n",
      "                                            |                                   \n",
      "token count: 8\n",
      "The information was entirely beyond their control.\n",
      "yes. both\n",
      "meaning checker result:yes. both\n",
      "found the one\n",
      "Sample: 5 =====================================================================\n",
      "\u001b[31mLabel: 1 (99.90%) --> Failed!\u001b[0m               |                                   \n",
      "                                            | Running Time:            1.272    \n",
      "                                            | Query Exceeded:          no       \n",
      "The data was completely out of [ members ’] | Victim Model Queries:    2        \n",
      "possession .”                               | BODEGA Score:            (later)  \n",
      "                                            | Succeed:                 no       \n",
      "                                            |                                   \n",
      "token count: 58\n",
      "Here is a paraphrased sentence with a similar length of 58 words:\n",
      "\n",
      "In contrast to President Obama's inaction, President Trump has taken a firm stance against Russia by approving the sale of offensive weapons to Ukraine, allowing them to counter Russian aggression, whereas his predecessor's weak response failed to provide adequate support to the Ukrainians in their time of need.\n",
      "yes. both\n",
      "meaning checker result:yes. both\n",
      "found the one\n",
      "Sample: 6 =====================================================================\n",
      "\u001b[32mLabel: 1 (95.74%) --> 0 (99.96%)\u001b[0m            |                                   \n",
      "                                            |                                   \n",
      "\u001b[1;31m    \u001b[0m \u001b[1;31m  \u001b[0m \u001b[1;31m \u001b[0m \u001b[1;31m           \u001b[0m \u001b[1;31m        \u001b[0m \u001b[1;31mBeyond\u001b[0m       |                                   \n",
      "\u001b[1;32mHere\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32ma\u001b[0m \u001b[1;32mparaphrased\u001b[0m \u001b[1;32msentence\u001b[0m \u001b[1;32mwith  \u001b[0m       |                                   \n",
      "                                            |                                   \n",
      "\u001b[1;31mpressuring\u001b[0m \u001b[1;31mour    \u001b[0m \u001b[1;31mallies\u001b[0m \u001b[1;31m, \u001b[0m \u001b[1;31mconsider\u001b[0m \u001b[1;31mthese\u001b[0m |                                   \n",
      "\u001b[1;32ma         \u001b[0m \u001b[1;32msimilar\u001b[0m \u001b[1;32mlength\u001b[0m \u001b[1;32mof\u001b[0m \u001b[1;32m58      \u001b[0m \u001b[1;32mwords\u001b[0m |                                   \n",
      "                                            |                                   \n",
      "\u001b[1;31mspecific\u001b[0m \u001b[1;31msteps\u001b[0m \u001b[1;31mPresident\u001b[0m \u001b[1;31mTrump\u001b[0m \u001b[1;31mhas      \u001b[0m    |                                   \n",
      "\u001b[1;32m:       \u001b[0m \u001b[1;32mIn   \u001b[0m \u001b[1;32mcontrast \u001b[0m \u001b[1;32mto   \u001b[0m \u001b[1;32mPresident\u001b[0m    |                                   \n",
      "                                            |                                   \n",
      "\u001b[1;31mtaken\u001b[0m \u001b[1;31magainst\u001b[0m \u001b[1;31mRussia\u001b[0m \u001b[1;31m:       \u001b[0m \u001b[1;31mWhere\u001b[0m         |                                   \n",
      "\u001b[1;32mObama\u001b[0m \u001b[1;32m'      \u001b[0m \u001b[1;32ms     \u001b[0m \u001b[1;32minaction\u001b[0m \u001b[1;32m,    \u001b[0m         |                                   \n",
      "                                            |                                   \n",
      "President \u001b[1;31mObama\u001b[0m \u001b[1;31mrefused\u001b[0m \u001b[1;31mto   \u001b[0m \u001b[1;31mprovide\u001b[0m       |                                   \n",
      "President \u001b[1;32mTrump\u001b[0m \u001b[1;32mhas    \u001b[0m \u001b[1;32mtaken\u001b[0m \u001b[1;32ma      \u001b[0m       |                                   \n",
      "                                            |                                   \n",
      "\u001b[1;31mserious\u001b[0m \u001b[1;31mweapons\u001b[0m \u001b[1;31mto     \u001b[0m \u001b[1;31mthe   \u001b[0m \u001b[1;31mUkrainians\u001b[0m   |                                   \n",
      "\u001b[1;32mfirm   \u001b[0m \u001b[1;32mstance \u001b[0m \u001b[1;32magainst\u001b[0m \u001b[1;32mRussia\u001b[0m \u001b[1;32mby        \u001b[0m   | Running Time:            3.4211   \n",
      "                                            | Query Exceeded:          no       \n",
      "\u001b[1;31mto       \u001b[0m \u001b[1;31mhelp\u001b[0m \u001b[1;31mthem\u001b[0m \u001b[1;31mdefend\u001b[0m \u001b[1;31mthemselves\u001b[0m       | Victim Model Queries:    3        \n",
      "\u001b[1;32mapproving\u001b[0m \u001b[1;32mthe \u001b[0m \u001b[1;32msale\u001b[0m \u001b[1;32mof    \u001b[0m \u001b[1;32moffensive \u001b[0m       | BODEGA Score:            (later)  \n",
      "                                            | Succeed:                 yes      \n",
      "\u001b[1;31m(      \u001b[0m \u001b[1;31mhis\u001b[0m \u001b[1;31mresponse\u001b[0m \u001b[1;31mwas\u001b[0m \u001b[1;31mweakness\u001b[0m \u001b[1;31mon  \u001b[0m \u001b[1;31ma \u001b[0m   |                                   \n",
      "\u001b[1;32mweapons\u001b[0m \u001b[1;32mto \u001b[0m \u001b[1;32mUkraine \u001b[0m \u001b[1;32m,  \u001b[0m \u001b[1;32mallowing\u001b[0m \u001b[1;32mthem\u001b[0m \u001b[1;32mto\u001b[0m   |                                   \n",
      "                                            |                                   \n",
      "\u001b[1;31mpathetic\u001b[0m \u001b[1;31mscale  \u001b[0m \u001b[1;31m),        \u001b[0m \u001b[1;31mPresident\u001b[0m       |                                   \n",
      "\u001b[1;32mcounter \u001b[0m \u001b[1;32mRussian\u001b[0m \u001b[1;32maggression\u001b[0m \u001b[1;32m,        \u001b[0m       |                                   \n",
      "                                            |                                   \n",
      "\u001b[1;31mTrump  \u001b[0m \u001b[1;31mhas\u001b[0m \u001b[1;31mapproved   \u001b[0m \u001b[1;31mthe\u001b[0m \u001b[1;31msale\u001b[0m \u001b[1;31mof  \u001b[0m       |                                   \n",
      "\u001b[1;32mwhereas\u001b[0m \u001b[1;32mhis\u001b[0m \u001b[1;32mpredecessor\u001b[0m \u001b[1;32m'  \u001b[0m \u001b[1;32ms   \u001b[0m \u001b[1;32mweak\u001b[0m       |                                   \n",
      "                                            |                                   \n",
      "\u001b[1;31moffensive\u001b[0m \u001b[1;31mweapons\u001b[0m to \u001b[1;31menable \u001b[0m \u001b[1;31mthe     \u001b[0m       |                                   \n",
      "\u001b[1;32mresponse \u001b[0m \u001b[1;32mfailed \u001b[0m to \u001b[1;32mprovide\u001b[0m \u001b[1;32madequate\u001b[0m       |                                   \n",
      "                                            |                                   \n",
      "\u001b[1;31mUkrainians\u001b[0m to \u001b[1;31m   \u001b[0m \u001b[1;31mincrease  \u001b[0m \u001b[1;31mthe\u001b[0m \u001b[1;31mcost \u001b[0m \u001b[1;31mof  \u001b[0m |                                   \n",
      "\u001b[1;32msupport   \u001b[0m to \u001b[1;32mthe\u001b[0m \u001b[1;32mUkrainians\u001b[0m \u001b[1;32min \u001b[0m \u001b[1;32mtheir\u001b[0m \u001b[1;32mtime\u001b[0m |                                   \n",
      "                                            |                                   \n",
      "\u001b[1;31mRussian\u001b[0m \u001b[1;31maggression\u001b[0m .                        |                                   \n",
      "\u001b[1;32mof     \u001b[0m \u001b[1;32mneed      \u001b[0m .                        |                                   \n",
      "                                            |                                   \n",
      "token count: 22\n",
      "Israelis and Saudi royals urge the U.S. to prevent Iran from establishing a land corridor from Tehran to Damascus and Lebanon.\n",
      "yes. the\n",
      "meaning checker result:yes. the\n",
      "found the one\n",
      "Sample: 7 =====================================================================\n",
      "\u001b[31mLabel: 0 (99.99%) --> Failed!\u001b[0m               |                                   \n",
      "                                            | Running Time:            2.348    \n",
      "The Israelis and Saudi royals want the U .  | Query Exceeded:          no       \n",
      "S . to keep Iran from securing a land       | Victim Model Queries:    2        \n",
      "bridge from Tehran to Damascus to Lebanon . | BODEGA Score:            (later)  \n",
      "                                            | Succeed:                 no       \n",
      "                                            |                                   \n",
      "token count: 20\n",
      "Senate opponents have turned their advisory role into a brutal, unrestricted smear campaign.\n",
      "yes. both\n",
      "meaning checker result:yes. both\n",
      "found the one\n",
      "Sample: 8 =====================================================================\n",
      "\u001b[31mLabel: 1 (95.04%) --> Failed!\u001b[0m               |                                   \n",
      "                                            | Running Time:            1.7741   \n",
      "His opponents in the Senate have            | Query Exceeded:          no       \n",
      "transformed their “ advise and consent ”    | Victim Model Queries:    2        \n",
      "function into a campaign of no - holds      | BODEGA Score:            (later)  \n",
      "barred character assassination .            | Succeed:                 no       \n",
      "                                            |                                   \n",
      "token count: 15\n",
      "We thought we knew this man, a supposed champion of traditional beliefs.\n",
      "yes. both\n",
      "meaning checker result:yes. both\n",
      "found the one\n",
      "Sample: 9 =====================================================================\n",
      "\u001b[31mLabel: 0 (99.23%) --> Failed!\u001b[0m               |                                   \n",
      "                                            | Running Time:            1.804    \n",
      "                                            | Query Exceeded:          no       \n",
      "This man whom we had believed a “ champion  | Victim Model Queries:    2        \n",
      "of orthodoxy ,” whom we thought we knew .   | BODEGA Score:            (later)  \n",
      "                                            | Succeed:                 no       \n",
      "                                            |                                   \n",
      "token count: 22\n",
      "This will render the monastery unsustainable as older members pass away, leaving only newcomers.\n",
      "yes. both\n",
      "meaning checker result:yes. both\n",
      "found the one\n",
      "Sample: 10 ====================================================================\n",
      "\u001b[32mLabel: 1 (92.65%) --> 0 (99.94%)\u001b[0m            |                                   \n",
      "                                            |                                   \n",
      "\u001b[1;31mIt  \u001b[0m will \u001b[1;31malso\u001b[0m \u001b[1;31mhelp  \u001b[0m the monastery \u001b[1;31mbecome\u001b[0m  |                                   \n",
      "\u001b[1;32mThis\u001b[0m will \u001b[1;32m    \u001b[0m \u001b[1;32mrender\u001b[0m the monastery \u001b[1;32m      \u001b[0m  |                                   \n",
      "                                            | Running Time:            1.7718   \n",
      "\u001b[1;31m‘\u001b[0m \u001b[1;31mless\u001b[0m \u001b[1;31mviable\u001b[0m \u001b[1;31m’\u001b[0m \u001b[1;31mbecause\u001b[0m \u001b[1;31mby\u001b[0m \u001b[1;31mthe          \u001b[0m    | Query Exceeded:          no       \n",
      "\u001b[1;32m \u001b[0m \u001b[1;32m    \u001b[0m \u001b[1;32m      \u001b[0m \u001b[1;32m \u001b[0m \u001b[1;32m       \u001b[0m \u001b[1;32m  \u001b[0m \u001b[1;32munsustainable\u001b[0m    | Victim Model Queries:    3        \n",
      "                                            | BODEGA Score:            (later)  \n",
      "\u001b[1;31mtime\u001b[0m \u001b[1;31manyone\u001b[0m \u001b[1;31myounger\u001b[0m \u001b[1;31mis  \u001b[0m \u001b[1;31mprofessed\u001b[0m , \u001b[1;31mthe\u001b[0m    | Succeed:                 yes      \n",
      "\u001b[1;32mas  \u001b[0m \u001b[1;32molder \u001b[0m \u001b[1;32mmembers\u001b[0m \u001b[1;32mpass\u001b[0m \u001b[1;32maway     \u001b[0m , \u001b[1;32m   \u001b[0m    |                                   \n",
      "                                            |                                   \n",
      "\u001b[1;31mothers\u001b[0m \u001b[1;31mwill   \u001b[0m \u001b[1;31mbe  \u001b[0m \u001b[1;31mdead     \u001b[0m \u001b[1;31m.”\u001b[0m            |                                   \n",
      "\u001b[1;32m      \u001b[0m \u001b[1;32mleaving\u001b[0m \u001b[1;32monly\u001b[0m \u001b[1;32mnewcomers\u001b[0m \u001b[1;32m. \u001b[0m            |                                   \n",
      "                                            |                                   \n",
      "token count: 10\n",
      "Asteroids and comets differ primarily in their chemical makeup.\n",
      "yes.\n",
      "meaning checker result:yes.\n",
      "found the one\n",
      "Sample: 11 ====================================================================\n",
      "\u001b[31mLabel: 0 (99.99%) --> Failed!\u001b[0m               |                                   \n",
      "                                            | Running Time:            1.7614   \n",
      "                                            | Query Exceeded:          no       \n",
      "The main difference between asteroids and   | Victim Model Queries:    2        \n",
      "comets is their composition .               | BODEGA Score:            (later)  \n",
      "                                            | Succeed:                 no       \n",
      "                                            |                                   \n",
      "token count: 52\n",
      "During a speech at the Foundation for Defense of Democracies, she cautioned against Iran's latest move to boost its nuclear enrichment capabilities, a bold response to Trump's withdrawal from the nuclear deal.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-584b376d0def>\u001b[0m in \u001b[0;36m<cell line: 119>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m     ])\n\u001b[1;32m    123\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattack_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_and_save_tsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvisualize_adv_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtsv_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmission_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0mattack_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-8e29bbaf0270>\u001b[0m in \u001b[0;36meval_and_save_tsv\u001b[0;34m(self, dataset, total_len, visualize, progress_bar, num_workers, chunk_size, tsv_file_path)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0;31m# Begin for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m           \u001b[0mtotal_inst\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m           \u001b[0msuccess_inst\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"success\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/OpenAttack/attack_eval/attack_eval.py\u001b[0m in \u001b[0;36mieval\u001b[0;34m(self, dataset, num_workers, chunk_size)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mattack_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattacker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvictim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_limit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/OpenAttack/attack_eval/attack_eval.py\u001b[0m in \u001b[0;36m__iter_metrics\u001b[0;34m(self, iterable_result)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0madversarial_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvoke_times\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             ret = {\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/OpenAttack/attack_eval/attack_eval.py\u001b[0m in \u001b[0;36mresult_iter\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mresult_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0mattack_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattacker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvictim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_limit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/OpenAttack/attack_eval/utils.py\u001b[0m in \u001b[0;36mattack_process\u001b[0;34m(attacker, victim, data, limit)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mattack_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvictim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exception when evaluate data %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/OpenAttack/attack_eval/utils.py\u001b[0m in \u001b[0;36mattack_process\u001b[0;34m(attacker, victim, data, limit)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvictim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0madversarial_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattacker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvictim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0minvoke_times\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvictim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mattack_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvictim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/OpenAttack/attackers/classification.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, victim, input_)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mgoal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifierGoal\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0morigin_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargeted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0madversarial_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvictim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madversarial_sample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-3839fae5f464>\u001b[0m in \u001b[0;36mattack\u001b[0;34m(self, victim, input_, goal)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mattack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvictim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparaphrase_with_llama\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx_new\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-3839fae5f464>\u001b[0m in \u001b[0;36mparaphrase_with_llama\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             mistral_out = replicate.run(\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0;34m\"mistralai/mistral-7b-instruct-v0.2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_mistral_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/replicate/client.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, ref, input, **params)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     async def async_run(\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/replicate/run.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(client, ref, input, **params)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"failed\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/replicate/prediction.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"succeeded\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"failed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"canceled\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "if using_mounted_drive:\n",
    "    data_path =  pathlib.Path(f\"{path_to_mounted_folder}/{task}\")\n",
    "    model_path = pathlib.Path(f\"{path_to_mounted_folder}/{task}/{victim_model}-512.pth\")\n",
    "    out_dir = pathlib.Path(f\"{path_to_mounted_folder}/outputs\")\n",
    "\n",
    "else:\n",
    "  data_path =  pathlib.Path(f\"/content/BODEGA/incrediblAE_public_release/{task}\")\n",
    "  model_path = pathlib.Path(f\"/content/BODEGA/incrediblAE_public_release/{task}/{victim_model}-512.pth\")\n",
    "  out_dir = pathlib.Path(\"/content/BODEGA/outputs\")\n",
    "\n",
    "\n",
    "\n",
    "RESULTS_FILE_NAME = 'results_' + task + '_' + str(targeted) + '_' + attack + '_' + victim_model + '.txt' #stores BODEGA metrics\n",
    "SUBMISSION_FILE_NAME = 'submission_' + task + '_' + str(targeted) + '_' + attack + '_' + victim_model + '.tsv' #stores original and modified text, to be submitted to shared task organizers\n",
    "\n",
    "results_path = out_dir / RESULTS_FILE_NAME if out_dir else None\n",
    "submission_path = out_dir / SUBMISSION_FILE_NAME if out_dir else None\n",
    "\n",
    "if out_dir:\n",
    "    if (out_dir / RESULTS_FILE_NAME).exists():\n",
    "      print(f\"Existing results file found. This script will overwrite previous file: {str(results_path)}\")\n",
    "    if submission_path.exists():\n",
    "      print(f\"Existing submission file found. This script will overwrite previous file: {str(submission_path)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare task data\n",
    "with_pairs = (task == 'FC' or task == 'C19')\n",
    "\n",
    "# Choose device\n",
    "print(\"Setting up the device...\")\n",
    "\n",
    "using_TF = (attack in ['TextFooler', 'BAE'])\n",
    "if using_TF:\n",
    "    # Disable GPU usage by TF to avoid memory conflicts\n",
    "    import tensorflow as tf\n",
    "\n",
    "    tf.config.set_visible_devices(devices=[], device_type='GPU')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('using GPU')\n",
    "    victim_device = torch.device(\"cuda\")\n",
    "    attacker_device = torch.device(\"cuda\")\n",
    "else:\n",
    "    victim_device = torch.device(\"cpu\")\n",
    "    attacker_device = torch.device('cpu')\n",
    "\n",
    "# Prepare victim\n",
    "print(\"Loading up victim model...\")\n",
    "if victim_model == 'BERT':\n",
    "    victim = VictimCache(model_path, VictimBERT(model_path, task, victim_device))\n",
    "elif victim_model == 'BiLSTM':\n",
    "    victim = VictimCache(model_path, VictimBiLSTM(model_path, task, victim_device))\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "test_dataset = Dataset.from_generator(readfromfile_generator,\n",
    "                                      gen_kwargs={'subset': 'attack', 'dir': data_path, 'trim_text': True,\n",
    "                                                  'with_pairs': with_pairs})\n",
    "if not with_pairs:\n",
    "    dataset = test_dataset.map(dataset_mapping)\n",
    "    dataset = dataset.remove_columns([\"text\"])\n",
    "else:\n",
    "    dataset = test_dataset.map(dataset_mapping_pairs)\n",
    "    dataset = dataset.remove_columns([\"text1\", \"text2\"])\n",
    "\n",
    "dataset = dataset.remove_columns([\"fake\"])\n",
    "\n",
    "# Filter data\n",
    "if using_first_n_samples:\n",
    "  dataset = dataset.select(range(first_n_samples))\n",
    "\n",
    "if targeted:\n",
    "    dataset = [inst for inst in dataset if inst[\"y\"] == 1 and victim.get_pred([inst[\"x\"]])[0] == inst[\"y\"]]\n",
    "\n",
    "print(\"Subset size: \" + str(len(dataset)))\n",
    "\n",
    "# Prepare attack\n",
    "print(\"Setting up the attacker...\")\n",
    "\n",
    "# Necessary to bypass the outdated SSL certifiacte on the OpenAttack servers\n",
    "with no_ssl_verify():\n",
    "  if using_custom_attacker:\n",
    "    attacker = LLAMA3StyleConverterAttack()\n",
    "  else:\n",
    "    filter_words = OpenAttack.attack_assist.filter_words.get_default_filter_words('english') + [SEPARATOR_CHAR]\n",
    "    if attack == 'PWWS':\n",
    "        attacker = OpenAttack.attackers.PWWSAttacker(token_unk=UNK_TEXT, lang='english', filter_words=filter_words)\n",
    "    elif attack == 'SCPN':\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "        attacker = OpenAttack.attackers.SCPNAttacker(device=attacker_device)\n",
    "    elif attack == 'TextFooler':\n",
    "        attacker = OpenAttack.attackers.TextFoolerAttacker(token_unk=UNK_TEXT, lang='english',\n",
    "                                                           filter_words=filter_words)\n",
    "    elif attack == 'DeepWordBug':\n",
    "        attacker = OpenAttack.attackers.DeepWordBugAttacker(token_unk=UNK_TEXT)\n",
    "    elif attack == 'VIPER':\n",
    "        attacker = OpenAttack.attackers.VIPERAttacker()\n",
    "    elif attack == 'GAN':\n",
    "        attacker = OpenAttack.attackers.GANAttacker()\n",
    "    elif attack == 'Genetic':\n",
    "        attacker = OpenAttack.attackers.GeneticAttacker(lang='english', filter_words=filter_words)\n",
    "    elif attack == 'PSO':\n",
    "        attacker = OpenAttack.attackers.PSOAttacker(lang='english', filter_words=filter_words)\n",
    "    elif attack == 'BERTattack':\n",
    "        attacker = OpenAttack.attackers.BERTAttacker(filter_words=filter_words, use_bpe=False, device=attacker_device)\n",
    "    elif attack == 'BAE':\n",
    "        attacker = OpenAttack.attackers.BAEAttacker(device=attacker_device, filter_words=filter_words)\n",
    "    else:\n",
    "        attacker = None\n",
    "\n",
    "# Run the attack\n",
    "print(\"Evaluating the attack...\")\n",
    "RAW_FILE_NAME = 'raw_' + task + '_' + str(targeted) + '_' + attack + '_' + victim_model + '.tsv'\n",
    "raw_path = out_dir / RAW_FILE_NAME if out_dir else None\n",
    "\n",
    "scorer = BODEGAScore(victim_device, task, align_sentences=True, semantic_scorer=\"BLEURT\", raw_path = raw_path)\n",
    "with no_ssl_verify():\n",
    "    attack_eval = BodegaAttackEval(attacker, victim, language='english', metrics=[\n",
    "        scorer  # , OpenAttack.metric.EditDistance()\n",
    "    ])\n",
    "    start = time.time()\n",
    "    summary = attack_eval.eval_and_save_tsv(dataset, visualize=visualize_adv_examples, progress_bar=False, tsv_file_path = submission_path)\n",
    "    end = time.time()\n",
    "attack_time = end - start\n",
    "attacker = None\n",
    "\n",
    "# Remove unused stuff\n",
    "victim.finalise()\n",
    "del victim\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "if \"TOKENIZERS_PARALLELISM\" in os.environ:\n",
    "    del os.environ[\"TOKENIZERS_PARALLELISM\"]\n",
    "\n",
    "# Evaluate\n",
    "start = time.time()\n",
    "score_success, score_semantic, score_character, score_BODEGA= scorer.compute()\n",
    "end = time.time()\n",
    "evaluate_time = end - start\n",
    "\n",
    "# Print results\n",
    "print(\"Subset size: \" + str(len(dataset)))\n",
    "print(\"Success score: \" + str(score_success))\n",
    "print(\"Semantic score: \" + str(score_semantic))\n",
    "print(\"Character score: \" + str(score_character))\n",
    "print(\"BODEGA score: \" + str(score_BODEGA))\n",
    "print(\"Queries per example: \" + str(summary['Avg. Victim Model Queries']))\n",
    "print(\"Total attack time: \" + str(attack_time))\n",
    "print(\"Time per example: \" + str((attack_time) / len(dataset)))\n",
    "print(\"Total evaluation time: \" + str(evaluate_time))\n",
    "\n",
    "if out_dir:\n",
    "  with open(results_path, 'w') as f:\n",
    "      f.write(\"Subset size: \" + str(len(dataset)) + '\\n')\n",
    "      f.write(\"Success score: \" + str(score_success) + '\\n')\n",
    "      f.write(\"Semantic score: \" + str(score_semantic) + '\\n')\n",
    "      f.write(\"Character score: \" + str(score_character) + '\\n')\n",
    "      f.write(\"BODEGA score: \" + str(score_BODEGA) + '\\n')\n",
    "      f.write(\"Queries per example: \" + str(summary['Avg. Victim Model Queries']) + '\\n')\n",
    "      f.write(\"Total attack time: \" + str(end - start) + '\\n')\n",
    "      f.write(\"Time per example: \" + str((end - start) / len(dataset)) + '\\n')\n",
    "      f.write(\"Total evaluation time: \" + str(evaluate_time) + '\\n')\n",
    "\n",
    "  print('-')\n",
    "  print('Bodega metrics saved to', results_path)\n",
    "  print('Submission file saved to', submission_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9S7Bmp72dxF"
   },
   "source": [
    "Your output should look like this.\n",
    "The custom attack has a very low BODEGA score, suggesting that the attack was not very successful (low success rate and low preservation of meaning).\n",
    "\n",
    "VictimBERT on PR2:\n",
    "```\n",
    "Subset size: 416\n",
    "Success score: 0.1778846153846154\n",
    "Semantic score: 0.40792732766351186\n",
    "Character score: 0.3001644500157\n",
    "BODEGA score: 0.02308437726605881\n",
    "Queries per example: 2.1778846153846154\n",
    "Total attack time: 19.421820878982544\n",
    "Time per example: 0.04668706942063112\n",
    "Total evaluation time: 10.617336988449097\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0axwkl8aBq5n"
   },
   "source": [
    "## Submission Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-FpmN3MBzfq"
   },
   "source": [
    "Whenever you run an attack on a dataset, a submission_task.tsv file will be saved to your outputs directory. At the end of the test phase, you will need to submit your final attack's submission files to the shared task organisers for evaluation (1 for each dataset * num_victim_classifiers).\n",
    "\n",
    "The submission file contains 4 pieces of information per attacked text:\n",
    "1. was the attack successful\n",
    "2. number of queries to victim model used to generate the adversarial sample\n",
    "3. the original text\n",
    "4. the adversarial text (or ATTACK_UNSUCCESSFUL if unsuccessful)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IZAXnoyI1hl"
   },
   "source": [
    "## Final tips:\n",
    "\n",
    "### Using a subset of eval dataset\n",
    "Testing your attack on the entire eval dataset can take a while. To speed things up, you can test on the first n samples of the dataset, by setting `using_first_n_samples` to `True`.  \n",
    "\n",
    "### Running pre-implemented attacks\n",
    "\n",
    "BODEGA supports a number of pre-existing attacks. Trying these might be useful if you want to:\n",
    "- compare your performance with existing methods (also reported in the [BODEGA preprint](https://arxiv.org/abs/2303.08032))\n",
    "- get inspiration from observing their substitutions\n",
    "\n",
    "To use an existing attack requires only two changes to the code above:\n",
    "1. set `using_custom_attacker` to `False`\n",
    "2. set `attack` to the name of a supported attack\n",
    "(`PWWS`, `SCPN`, `TextFooler`, `DeepWordBug`, `GAN`, `Genetic`, `PSO`, `BERTattack` or`BAE`)\n",
    "\n",
    "Note that using `BAE` or `TextFooler` will require you to install additional dependencies since they rely on tensorflow:\n",
    "\n",
    "- tensorflow >= 2.0.0\n",
    "- tensorflow_hub\n",
    "\n",
    "https://openattack.readthedocs.io/en/latest/quickstart/installation.html\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Ys5EJ9gPnWYy"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
