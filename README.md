# CheckThat!2024 Task 6: Robustness of Credibility Assessment with Adversarial Examples (InCrediblAE)

This repository contains the code and resources developed for **Task 6: Robustness of Credibility Assessment with Adversarial Examples (InCrediblAE)** as part of the CLEF 2024 competition. Our team participated in this task to evaluate the robustness of popular text classification models against adversarial attacks.
You can find our paper [here](https://ceur-ws.org/Vol-3740/paper-35.pdf).

## Task Overview

The goal of the task is to generate adversarial examples for five problem domains to assess the robustness of three victim classifiers. These examples must:

1. Alter the decision of victim classifiers (confusion score).
2. Preserve the semantic meaning of the text (semantic score).
3. Maintain a high similarity to the original text in terms of characters (character score).

The evaluation of adversarial examples is based on the **BODEGA framework**, and submissions are ranked by their average BODEGA scores across all domains and classifiers.

### Problem Domains
The task includes the following domains:

- **Style-based news bias assessment**
- **Propaganda detection**
- **Fact checking**
- **Rumour detection**
- **COVID-19 misinformation detection**

### Victim Classifiers
Participants create adversarial examples to target three victim classifiers:

- **Fine-tuned BERT**
- **BiLSTM**
- **Surprise classifier - reveled after the competition as RoBERTa**

## Repository Contents

- **`IncrediblAE_space.ipynb`**: The primary Jupyter Notebook used for generating adversarial examples. This notebook builds on the provided BODEGA framework and includes the implementation of our custom attack strategy.
- **`data/`**: Placeholder for the attack datasets provided for the competition.
- **`results/`**: Output files containing adversarial examples for each scenario (to be included in the final submission).

## Getting Started

### Prerequisites

To run the notebook, you need the following:

- Python 3.8+
- Libraries including `transformers`, `torch`, `numpy`, `pandas`, `scipy`, `BLEURT`, and others as specified in the Colab notebook.

Install the necessary libraries by running the following command in your environment:

```bash
pip install transformers torch numpy pandas scipy
```
Other dependencies such as BLEURT may require specific installation instructions. Refer to the relevant documentation if needed.

### Usage

1. Clone this repository and navigate to the directory.
2. You can find the codes of attackers written in the paper under `submitted_attackers` folder. Open any file using Jupyter Notebook or Colab.
3. Follow the steps outlined in the notebook:
   - Load the attack datasets.
   - Implement your attack strategy by modifying the `MyAttacker` class.
   - Generate adversarial examples for each domain and victim classifier.
   - Save the outputs in the format required for submission.

## Evaluation Metrics

The BODEGA score evaluates the quality of adversarial examples based on:

- **Confusion Score**: Whether the victim classifierâ€™s decision changes.
- **Semantic Score**: Semantic similarity measured using BLEURT.
- **Character Score**: Text similarity based on Levenshtein distance.

## References

- [CLEF 2024 Task 6 Details](https://checkthat.gitlab.io/clef2024/task6/)
- [BODEGA Repository](https://github.com/piotrmp/BODEGA)
- [GitLab Repository for Task Data](https://gitlab.com/checkthat_lab/clef2024-checkthat-lab/-/tree/main/task6?ref_type=heads)

## Acknowledgements

This work was developed as part of the CLEF 2024 competition. We thank the organizers for providing the datasets, evaluation framework, and valuable guidance.

## Citation
If you use this repository or the ideas presented in the paper, please cite our work as follows:

```
@inproceedings{demirok2024turquaz,
  title={TurQUaz at CheckThat! 2024: Creating Adversarial Examples using Genetic Algorithm},
  author={Demirok, B., Mergen, S., Oz, B., & Kutlu, M.},
  booktitle={Conference and Labs of the Evaluation Forum},
  year={2024},
  publisher={CEUR Workshop Proceedings},
  url={https://ceur-ws.org/Vol-3740/paper-35.pdf}
}
```
